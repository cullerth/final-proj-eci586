---
title: "Final Project 586"
author: "Tori Culler"
date: "11/30/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidytext)
library(wordcloud2)
library(forcats)
library(dplyr)
library(readtext) 
library(ggplot2)
library(SnowballC)
library(topicmodels)
library(stm)
library(ldatuning)
library(knitr)
library(LDAvis)
library(stringr)
library(XML)
library(xml2)
library(lubridate)

#import all chats, raw
chats <- read_csv("data/chats.csv")

#omit empty chats
chats_text <- na.omit(chats)

#chats came in the from of xml in one column of the csv, need to loop through them and put them in a more human readable form
#set empty vector to catch only text from chats
transcript_text <- c()

#loop through chats, transform into plain text instead of xml, use regular expressions to remove the system messages and any links
#append to transcript_text vector
for(transcript in chats_text$xml)
{
  doc = xmlTreeParse(transcript, asText = TRUE, useInternalNodes = TRUE)
  chats_message_text <- xmlValue(doc["//message"])
  chats_message_text <- str_replace(chats_message_text, "^System message:.*", "")
  chats_message_text <- str_replace(chats_message_text, "<a\\s*.*>\\s*.*<\\/a>", "")
  chats_message_text <- str_replace(chats_message_text, "https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()!@:%_\\+.~#?&\\/\\/=]*)", "")
  transcript_text = c(transcript_text, paste(chats_message_text, collapse=" "))
}

#bind transcript_text vector to chats_text dataframe as new column
chats_text <- cbind(chats_text, data.frame(transcript_text))

#split 'started' column into two different columns for date and time
chats_text[c('date','time')] <- str_split_fixed(chats_text$started, " ", 2)

#reorder columns and filter for only those of interest
chats_text_final <- chats_text %>% 
  select(id, date, time, wait, duration, referrer, transcript_text)

#create a new dataset that just contains chats for the spring 2021 semester
chats_text_spring2021 <- chats_text_final[(chats_text_final$date> "2021-01-09" & chats_text_final$date < "2021-05-06"),]

#time to tidy.
#chats overall
chats_tidy <- chats_text_final %>% 
  unnest_tokens(word, transcript_text) %>%
  anti_join(stop_words)

#spring 2021
chats_tidy_spring2021 <- chats_text_spring2021 %>% 
  unnest_tokens(word, transcript_text) %>%
  anti_join(stop_words)


```
## I. PREPARE

For my final project, I was interested in exploring the transcripts of chats from our AskUs service at NC State University Libraries. 

<center>
![](images/ask_us_logo.png){width=25%}
</center>

# Background

I had long heard others in the Libraries mention that exploring our chat transcripts using data mining methods could be a worthwhile endeavor. After reading an article out of Carnegie Mellon by Xiaoju & Wang entitled ["Automated Chat Transcript Analysis using Topic Modeling for Library Reference Services"](https://asistdl-onlinelibrary-wiley-com.prox.lib.ncsu.edu/doi/full/10.1002/pra2.31), I was inspired to tackle the challenge. 

The goal of Xiaoju & Wang’s research was to identify the sorts of topics that emerge on chat in order to better understand patrons’ needs and improve reference services and new librarian training procedures. My goal here was to more or less replicate the Xiaoju & Wang study using basic word counts, n-grams, and topic modeling to reach the same ends. 

I was also wanted to expand somewhat on the work that of Xiaoju & Wang by using sentiment analysis to see if more positive or negative emotions were represented overall in our chat transcripts. 

In their study, Xiaoju & Wang saw 8 main topics emerge: "Physical Book Access", "Journal Article Access", "Off Campus Access", "Interlibrary Loan", "Specialized Reference", "Guest Access", "Thesis & Dissertation", and "http link to catalog item". To broadly summarize these topics, the theme is 'how to gain access to library resources', which is absolutely what you'd expect to show up in chat transcripts.

<center>
![](images/xiaoju&wang_topics.png)
</center>

I hypothesized that what I found would be very similar, but I did anticipate that we might have more specific NCSU flavored topics related to some of our unique spaces and services -- that things like ‘tech lending’, ‘textbook lending’, ‘virtual reality’, ‘makerspace’, or ‘data visualization’ might show up, for example. 

Whatever the findings, I was hopeful that the results would give the Libraries a clearer picture of how patrons are using chat, which will only help us to improve that service and better train new staff on what to expect and how to interact with patrons effectively. 

# Research Questions

Things I was interested in knowing include:   

- What are the most common topics that our patrons seek help for on chat?  
- What sorts of emotional sentiments show up most often in chat?   
- Are there any meaningful differences in what patrons ask over time (both longitudinally across the period studied and/or within the rhythm of a day, week, or semester)?  

# Methods

- Basic word counts
- Topic modeling using LDA  
- Sentiment analysis w/ NRC  
- Bi-grams + tri-grams  
- Selected quotes using filter() + grepl()

## II. WRANGLE 

# Pre-processing Steps

1. Firstly, I read in the csv of our chat transcripts as a dataframe. I requested transcripts for the past 5 years, and our User Experience department delivered! After omitting empty values, I found myself with 54,458 chats to work with spanning from May 3rd 2017 - May 3rd 2021.To start, I had 13 different variables to work with: 

```{r, eval=T, echo=T}
colnames(chats)
```


2. I quickly discovered that the chat transcripts were in that final xml column and included a lot of extraneous information that I needed to get rid of before moving on to tokenization and tidying: 

```{r, eval=F, echo=T}
#loop through chats, transform into plain text instead of xml
#use regular expressions to remove the system messages and any links
#append to transcript_text vector
#add transcript_text vector as new column to dataframe 
transcript_text <- c()

for(transcript in chats_text$xml)
{
  doc = xmlTreeParse(transcript, asText = TRUE, useInternalNodes = TRUE)
  chats_message_text <- xmlValue(doc["//message"])
  chats_message_text <- str_replace(chats_message_text, "^System message:.*", "")
  chats_message_text <- str_replace(chats_message_text, "<a\\s*.*>\\s*.*<\\/a>", "")
  chats_message_text <- str_replace(chats_message_text, "https?:\\/\\/(www\\.)?[-a-zA-Z0-9@:%._\\+~#=]{1,256}\\.[a-zA-Z0-9()]{1,6}\\b([-a-zA-Z0-9()!@:%_\\+.~#?&\\/\\/=]*)", "")
  transcript_text = c(transcript_text, paste(chats_message_text, collapse=" "))
}

chats_text <- cbind(chats_text, data.frame(transcript_text))
```

3. But I still wasn't ready to move onto tokenization -- I also decided to break up the 'started' column into two distinct 'date' and time' columns. I also selected the columns that I was interested in working with at this point. 

```{r, eval=F, echo=T}
#split 'started' column into two different columns for date and time
chats_text[c('date','time')] <- str_split_fixed(chats_text$started, " ", 2)

#reorder columns and filter for only those of interest
chats_text_final <- chats_text %>% 
  select(id, date, time, wait, duration, referrer, transcript_text)
```

4. And NOW it was time to tokenize and tidy the text, removing stopwords such as 'a' 'and' and 'the':

```{r, eval=F, echo=T}
chats_tidy <- chats_text_final %>% 
  unnest_tokens(word, transcript_text) %>%
  anti_join(stop_words)
```

5. So that I could look at a subset of the data if I needed or wanted to instead of the massive 5 year dataframe all at once, I created a smaller dataframe for just the Spring 2021 semester: 

```{r, eval=F, echo=T}
chats_text_spring2021 <- chats_text_final[(chats_text_final$date> "2021-01-09" & chats_text_final$date < "2021-05-06"),]

chats_tidy_spring2021 <- chats_text_spring2021 %>% 
  unnest_tokens(word, transcript_text) %>%
  anti_join(stop_words)
```

5. I also stemmed and 'chunked' the text for easier topic modeling -- see that section for more info. 

## III. EXPLORE & MODEL 

# Summary Stats for Time Variables

While I was here, and before jumping into the text mining techniques, I also wanted to look at some summary stats for the time variables that were available; 'wait' and 'duration'. *'Wait'* refers to the amount of time that a patron would spend, well, waiting for their chat to be picked up. And *'duration'* refers to the amount of time that it takes to resolve a chat. 

Here's a quick look at how long patrons spend **waiting** for their chat to be picked up, based on the larger dataframe containing points for the past 5 years. 

- For the most part, it looks like things get picked up pretty quickly, with the median wait time being 14 seconds and the average wait time being about 24 seconds.  
- Plotting the wait times as a normal distribution, it's clear that there is just a really long tail.   
- Zooming in, you can see that the vast majority of chats are taken in under 150 seconds, or about 2.5 minutes. 


```{r, eval=T, echo=F}

#summary stats wait
overall_descriptives_wait <- chats_tidy %>%
  summarise(min_duration = seconds_to_period(min(chats_tidy$wait)),
            max_duration = seconds_to_period(max(chats_tidy$wait)),
            median_duration = seconds_to_period(median(chats_tidy$wait)),
            mean_duration = seconds_to_period(mean(chats_tidy$wait))
  )

overall_descriptives_wait 

#wait distribution with max
x = seq(0, 3270, by=5)

y = dnorm(x, mean(chats_tidy$wait), sd(chats_tidy$wait))

plot(x, y)

#wait distribution zoomed in
x = seq(0, 300, by=5)

y = dnorm(x, mean(chats_tidy$wait), sd(chats_tidy$wait))

plot(x, y)



```

And now a peak at the same stats for **duration**, or how long patrons spend chatting with a librarian before navigating away from the chat windo. This is, again, based on the larger dataframe containing points for the past 5 years. 

- The median time patrons spend in a chat session is about 8 minutes, and the average is at about 12 minutes.  
- Plotting the durations as a normal distribution, a super long tail also emerges here.   
- Zooming in, you can see that the vast majority of chats are taken in under 3000 seconds, or about 50 minutes. This might seem like a long time, but as someone who has worked the chat before I can say that that sounds about right. 12 minutes does seem about average, and sometimes chats can go on for a while if the patron themselves isn't responding or if it's just a length issue to resolve.  

```{r, eval=T, echo=F}

#summary stats duration
overall_descriptives_duration <- chats_tidy %>%
  summarise(min_duration = seconds_to_period(min(chats_tidy$duration)),
           max_duration_mins = seconds_to_period(max(chats_tidy$duration)),
           median_duration = seconds_to_period(median(chats_tidy$duration)),
           mean_duration = seconds_to_period(mean(chats_tidy$duration))
  )

overall_descriptives_duration

#duration distribution with max
x = seq(0, 10360, by=100)

y = dnorm(x, mean(chats_tidy$duration), sd(chats_tidy$duration))

plot(x, y)

#duration distribution zoomed in
x = seq(0, 5000, by=100)

y = dnorm(x, mean(chats_tidy$duration), sd(chats_tidy$duration))

plot(x, y)


```

Pretty impressive overall I'd say, I'll pause now for a round of applause for our Ask Us staff!

![](images/ask_us_smiley.jpeg)


Okay, now for the text mining that we all came here for: 

# Basic Word Counts

A look at the basic word counts for the **past 5 years** produces...about what one would expect. "Library" turns out to be the most used word (who would have guessed!?). It's followed by the words, "check", "book", "access", "request", "hunt", and "hill". 

```{r, eval=T, echo=F}
# basic word counts
word_counts_overall <- chats_tidy %>%
  count(word, sort = TRUE) %>%
  ungroup()

head(word_counts_overall, n=20)

#wordcloud
wordcloud2(word_counts_overall,
           color = ifelse(word_counts_overall[, 2] > 20000, 'black', 'gray'))

#bar graph 
word_counts_overall %>%
  filter(n > 9800) %>% 
  mutate(word = reorder(word, n)) %>% 
  ggplot(aes(n, word)) + #
  geom_col() +
  labs(x = "Word Counts", y = NULL) + 
  theme_minimal()
```

The results for **Spring 2021** are pretty similar, with "library" still coming in the top slot, and the next few words being "request", "book", "access", and "link".

```{r, eval=T, echo=F}
#basic word count
word_counts_spring2021 <- chats_tidy_spring2021 %>%
  count(word, sort = TRUE) %>%
  ungroup()

head(word_counts_spring2021, n=20)

#word cloud
wordcloud2(word_counts_spring2021,
           color = ifelse(word_counts_spring2021[, 2] > 600, 'black', 'gray'))

#bar graph
word_counts_spring2021 %>%
  filter(n > 482) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) + #
  geom_col() +
  labs(x = "Word Counts", y = NULL) +
  theme_minimal()
```

# Bi-grams & Tri-grams

Overall, I didn't find the word counts to be terribly insightful. 

Thinking I might have better luck with bi-grams (i.e. -- 2 words that appear next to each other) & trigrams (i.e. -- 3 words that appear next to each other), I turned there next. 

Looking at the bi-grams for the entire dataset, here's what we see as the top 20: 

```{r, eval=T, echo=F}
overall_bigrams <- chats_text %>%
  unnest_tokens(bigram, transcript_text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  unite(bigram, word1, word2, sep = " ") %>% 
  count(bigram, sort = TRUE)

head(overall_bigrams, n=20)
```

Pretty similar to what we see when we look at the top 20 bi-grams for Spring 2021. Though, I will point out that tech lending is higher on this list -- which I would presume is an effect of the pandemic, during which time we have greatly ramped up our tech lending services: 

```{r, eval=T, echo=F}
spring2021_bigrams <- chats_text_spring2021 %>%
  unnest_tokens(bigram, transcript_text, token = "ngrams", n = 2) %>%
  separate(bigram, c("word1", "word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word) %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(bigram, sort = TRUE)

head(spring2021_bigrams, n=20)
```

And then just for kicks, let's check out tri-grams for the whole dataset: 

```{r, eval=T, echo=F}
overall_trigrams <- chats_text %>%
  unnest_tokens(trigram, transcript_text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  unite(trigram, word1, word2, word3, sep = " ") %>% 
  count(trigram, sort = TRUE)

head(overall_trigrams, n=20)
```

And for Spring 2021: 

```{r, eval=T, echo=F}
spring2021_trigrams <- chats_text_spring2021 %>%
  unnest_tokens(trigram, transcript_text, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  unite(trigram, word1, word2, word3, sep = " ") %>%
  count(trigram, sort = TRUE)

head(spring2021_trigrams, n=20)
```

Something that did catch my eye when I started digging into the tri-grams for Spring 2021 specifically is that the phrase, 'mental health services' was in the no. 26 slot of the most commonly used tri-grams, with 9 mentions. 

I definitely wanted to investigate this further, as I initially read it as: "Wow, it's been such a tough year/semester for folks with covid and all that it has wrought that we have people reaching out to the Libraries asking for referrals to mental health services." This would definitely be something worth addressing if this were the case.

But, upon searching for that phrase in the chats to see where it appeared, it became clear that this was actually a completely bogus assumption. Every chat in which the phrase 'mental health services' appeared was apparently in reference to a specific assignment from a social work course in which students were asked to find scholarly sources about the nature of mental health services for specific populations.

```{r, eval=F, echo=T}
spring2021_mh <- chats_text_spring2021 %>%
  select(transcript_text) %>%
  filter(grepl('mental health services', transcript_text))

spring2021_mh<- sample_n(spring2021_mh, 5)

spring2021_mh
```
> "Hello I am looking for Scholarly articles for Sw 501 Class. My topic is about the LGBTQIA Community far as be able access **mental health services**..."

> "Hello I am trying look for articles in Sw 510 classes on Ethical consideration of African - American Women using **mental health services**..."

> "Hello I am need some help on find scholarly articles on the LGBTQIA Community on how they not able access **mental health services**..."

> "Hello I am trying look up articles on expriences of black women have with **mental health services** this for the class Sw 510..."

> "I was also looking for article physician who are cultural competency within lgbtqia community for **mental health services**..."

The lesson here? Take all this text mining stuff with a grain of salt. It's an extra tool in our kits, not a panacea. 

# Topic Modeling with LDA

Next up, moving on from the basic word counts and n-grams, is my dive into topic modeling. 

Initially, I tried to do topic modeling for the overall dataset, which spans the past 5 years. It quickly became clear, however, that my laptop isn't powerful enough to handle all of that. 

So what follows, then, is topic modeling for just the Spring 2021 subset of the data. 

If this were to prove useful to folks in the Libraries, I would be interested in using some of the more powerful machines we have in our very own [Dataspace](https://www.lib.ncsu.edu/spaces/dataspace) or [Data Experience Lab](https://www.lib.ncsu.edu/spaces/dxl) to carry out some of the topic modeling for the larger dataset. 

At any rate, before I dive into topic modeling, let's review the definition of topic modeling and LDA as described in [*Text Mining with R*](https://www.tidytextmining.com/topicmodeling.html): 

> "Topic modeling is a method for unsupervised classification of such documents, similar to clustering on numeric data, which finds natural groups of items even when we’re not sure what we’re looking for.

> Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. This allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups, in a way that mirrors typical use of natural language."

To begin, I did a bit more pre-processing, including stemming (which accounts for the various forms of words -- reducing the words likes, liked, likely, and liking to 'like', for example) and the creation of a document term matrix (one document [i.e., chat, in this case] per word per row).

```{r, eval=T, echo=T}
# counting words by chat, where each individual chat is considered to be a document, to create a document term matrix
word_counts_spring2021_dtm <- chats_tidy_spring2021 %>%
  count(id, word, sort = TRUE) %>%
  ungroup()

#casting document term matrix
chats_dtm_spring2021 <- word_counts_spring2021_dtm %>%
  cast_dtm(id, word, n)

#further text processing
temp <- textProcessor(chats_text_spring2021$transcript_text,
                      metadata = chats_text_spring2021,
                      lowercase=TRUE,
                      removestopwords=TRUE,
                      removenumbers=TRUE,
                      removepunctuation=TRUE,
                      wordLengths=c(3,Inf),
                      stem=FALSE,
                      onlycharacter= FALSE,
                      striphtml=TRUE,
                      customstopwords=FALSE)
meta <- temp$meta
vocab <- temp$vocab
docs <- temp$documents

#stemming the text
stemmed_chats_spring2021 <- chats_text_spring2021 %>%
  unnest_tokens(output = word, input = transcript_text) %>%
  anti_join(stop_words, by = "word") %>%
  mutate(stem = wordStem(word))

#combining it all back together
stemmed_dtm_spring2021 <- chats_text_spring2021 %>%
  unnest_tokens(output = word, input = transcript_text) %>%
  anti_join(stop_words, by = "word") %>%
  mutate(stem = wordStem(word)) %>%
  count(word, stem, sort = TRUE) %>%
  cast_dtm(word, stem, n)
```

Next, I ran it through an LDA tool & visualizer to fit it all to 8 topics. The K one chooses is completely arbitrary -- I selected 8 to start because that's how many topics Xiaoju & Wang used in their article. 

```{r, eval=T, echo=T}
#running lda to find 8 topics
chats_lda_spring2021 <- LDA(chats_dtm_spring2021, k = 8, control = list(seed = 1234))

# #seeing it visualized
# toLDAvis(mod = stm_spring_2021, docs = docs)
```

Here is the visualization that it produced, honing in on topic number 8. I think it's interesting that topic 8 is sitting out there all by its lonesome, though upon looking at the most salient terms I see that 'covid' is listed and a number of other words that make me think this topic isn't one that would appear all the time...this topic could be an outlier that is mostly having to do with inquiries around updated policies and procedures at the Libraries due to covid. 

<center>
![](images/topic_8_LDAvis.png)
</center>
And let's see the top 8 topics listed with the top 10 words associated with each using STM (the Structural Topic Model package), so that I might try to create my own version of the table in the Xiaoju & Wang article: 

```{r, eval=T, echo=T}
#another way of finding topics
stm_spring_2021 <- stm(documents=docs,
                  data=meta,
                  vocab=vocab,
                  K=8,
                  max.em.its=25,
                  verbose = FALSE)

#seeing it visualized, listing top 10 words associated with each topic
#plot.STM(stm_spring_2021, n = 10)
```
<center>
![](images/stm-topics.png)
</center>

Using this as my basis, here is the table I generated that includes my own categorization of each of those topics. I would be interested to see if my colleagues who work chat regularly would agree with my categories, and if it feels accurate to their experiences with chat: 


<center>
![](images/ncsu-spring2021-topics.png)
</center>

(It's worth noting that there is, in fact, a way to estimate the 'optimal' value of K/number of topics you should set. When I ran the Spring 2021 data through that function, you can see below that it suggested that I set as many as 25, 30, or 40 topics. 

This seemed rather unweildy to me, and given what I had found so far, unlikely to add any significant meaning to the analysis, so I opted to leave it at 8 topics.) 

```{r, eval=F, echo=F}
#a way of finding the supposed 'optimal' k value
k_metrics_spring2021 <- FindTopicsNumber(
  chats_dtm_spring2021,
  topics = seq(10, 75, by = 5),
  metrics = "Griffiths2004",
  method = "Gibbs",
  control = list(),
  mc.cores = NA,
  return_models = FALSE,
  verbose = FALSE,
  libpath = NULL
)

#plotting optimal k
FindTopicsNumber_plot(k_metrics_spring2021)
#and it says either 25, 30, or 45...which seems unwieldy, and my laptop has trouble running that
```

<center>
![](images/finding-k-spring2021.png)
</center>

# Sentiment Analysis

Finally, let's do some sentiment analysis. 

For this protion, I used the [NRC lexicon](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm). 

> "The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive). The annotations were manually done by crowdsourcing."


```{r, eval=T, echo=T}
#loading the lexicon
nrc <- get_sentiments("nrc")

#matching words to sentiments for spring 2021
sentiment_nrc_spring2021 <- inner_join(word_counts_spring2021, nrc, by = "word")

summary_nrc_spring2021 <- sentiment_nrc_spring2021 %>%
  count(sentiment, sort = TRUE) %>%
  spread(sentiment, n) %>%
  mutate(sentiment = positive - negative) %>%
  mutate(lexicon = "nrc") %>%
  relocate(lexicon)

#seeing counts
nrc_counts <- sentiment_nrc_spring2021 %>%
  count(sentiment, sort = TRUE)

#graphed
nrc_counts %>%
  mutate(sentiment = reorder(sentiment,n)) %>%
  ggplot(aes(n, sentiment)) +
  geom_col() +
  labs(x = "NRC Sentiment", y = NULL) +
  theme_minimal()

```


Overall, there are more positive than negative sentiments expressed. I am also encouraged that trust is the third most frequent sentiment, and unsurprised that anticipation is the fourth most frequent (people are requesting assistance and in this context usually expect quick resolution, so, makes sense).

## IV. COMMUNICATE

## Conclusions, Implications, Limitations

**Conclusions**    
In the end, I would say that I was fairly successful in replicating the Xiaoju & Wang analysis. Let's return once again to my research questions:

- What are the most common topics that our patrons seek help for on chat?  
- What sorts of emotional sentiments show up most often in chat?   
- Are there any meaningful differences in what patrons ask over time (both longitudinally across the period studied and/or within the rhythm of a day, week, or semester)?  

*Most Common Topics*    
Using basic word counts, n-grams, topic modeling, and sentiment analysis, I was able to answer my first research question by gaining an overall picture of the kinds of requests patrons come to our chat service with. 

The topics that emerged included: "Interlibrary Loan", "Course Reserves/Textbook Lending",
"How to Renew Books Online", "Summon Redirects/Journal Article Access", 
"Research Support", "Space/Computing Assistance", and "General Library Access".

I hypothesized that the topics that emerged in my analysis based on  NC State University Libraries would have a different overall flavor unique to some of our particular services than what Xiaoju & Wang found at Carnegie Mellon, and I was correct in that. 

*Sentiments*    
Additionally, I was able to use NRC to do some basic sentiment analysis for the spring 2021 semester. This indicated that sentiments were more overall positive than negative while also being characterized by feelings of trust and anticipation.

*Changes Over Time*    
As I will get into below, I was unfortunately not able to answer my last research question about changes in topics of interest over time. 

**Limitations**    
There were a couple of notable roadblocks I ran into with the capabilities of my machine that prevented me from doing the full analysis I had wanted to do. Because I wasn't able to run topic models for the entire dataset, for example, I was resigned to looking at topics for a single semester. Furthermore, if I wanted to look at topics over time, I would need to replicate my work several times over for many semesters. I elected not to do so at present until I am able to determine if this kind of analysis is helpful for the Libraries. 

Based on the results of my topic modeling, it is also clear that I could have done some additional pre-processing to make the outputs even cleaner. There was lots of back and forth polite filler that dominates a lot of the chats, for instatnce. I would look to remove that in the future if I were to continue on. 

**Implications**    
I do question how useful this kind of analysis is for our work at the Libraries. It is very time and labor intensive to do this sort of exploration, and I am not sure that it tells us much more than what we might be able to glean in other ways. How does what I found here compare to any kind of internal analytics that might be available from our chat service, LibraryH3lp? Surely they have similar stats on the average wait times and durations of chats, for example? And is it actually worthwhile to run these computations only to uncover topics that the humans who staff our chat service everyday probably could have brainstormed on their own?

I am reminded of what Dr. Alyssa Wise dubs as the "5 P's" in her talk ["What is Learning Analytics?"]( https://www.youtube.com/watch?v=KpGtax2RBVY&feature=youtu.be); the questions that one should ask before investing in any kind of large scale Learning Analytics project -- What is the Point, Proxy, Process, Plan, Payoff. Asking these questions helps to avoid collecting and analyzing data that is ‘good to know’ but not really actionable.

What I found here may fall into more of the 'good to know' category, but if I had to take a stab at some actionable points that could be buried in here, one thing I might latch onto is the topics I uncovered. One in particular stood out to me: "How to Renew Books Online". If this was indeed what that topic represented, then I would wager that perhaps there is work there that we could do to make that information more clear to patrons on our website or in our reminder emails (even though I think we already do a pretty good job of that) so that they don't have to reach out to us via chat to get those details (even though I think we already do a pretty good job of that). 

At any rate, I look forward to showing what I've begun here to colleagues in the Libraries whose work is more closely tied to our chat service to get their thoughts on potential applications of/improvements to this work. 

## *References*
Chen, Xiaoju and Huajin Wang. ["Automated Chat Transcript Analysis using Topic Modeling for Library Reference Services."](https://asistdl-onlinelibrary-wiley-com.prox.lib.ncsu.edu/doi/full/10.1002/pra2.31) Proceedings of the Association for Information Science and Technology 56, no. 1 (2019): 368-371.

Mohammad, S., 2021. NRC Emotion Lexicon. [online] saifmohammad.com. Available at: <https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm> [Accessed 1 December 2021].

Silge, J., & Robinson, D. (2017). [Text mining with R: A tidy approach](https://www.tidytextmining.com/topicmodeling.html). " O'Reilly Media, Inc.". 

Wise, Alyssa.(2021, July 7). What is Learning Analytics? [Video]. YouTube. https://www.youtube.com/watch?v=KpGtax2RBVY&feature=youtu.be





